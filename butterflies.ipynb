{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tfm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader \n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset comes from: https://www.kaggle.com/gpiosenka/butterfly-images40-species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the mean and std for ImageNet dataset\n",
    "Mean = np.asarray([ 0.485, 0.456, 0.406 ])\n",
    "Std = np.asarray([ 0.229, 0.224, 0.225 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tfm.Compose([tfm.RandomCrop(224, padding=4, padding_mode='reflect'), \n",
    "                         tfm.RandomHorizontalFlip(0.5),                          \n",
    "                         tfm.RandomVerticalFlip(0.6),\n",
    "                         tfm.ToTensor(), \n",
    "                         tfm.Normalize(mean=Mean, std=Std, inplace=True), # normalize image based on mean and std of ImageNet dataset\n",
    "                         tfm.Lambda(lambda x: x.mul(255.))\n",
    "                         ])\n",
    "test_tfms = tfm.Compose([\n",
    "    tfm.ToTensor(),\n",
    "    tfm.Normalize(mean=Mean, std=Std, inplace=True), # normalize image based on mean and std of ImageNet dataset\n",
    "    tfm.Lambda(lambda x: x.mul(255.))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "load_tfms = tfm.Compose([\n",
    "    tfm.Resize((224,224)), \n",
    "    tfm.ToTensor(),\n",
    "    tfm.Normalize(Mean, Std, inplace=True), # normalize image based on mean and std of ImageNet dataset\n",
    "    tfm.Lambda(lambda x: x.mul(255.))\n",
    "    ])\n",
    "\n",
    "def loadPicture(path):    \n",
    "    image = Image.open(path).convert('RGB')    \n",
    "    img=load_tfms(image)    \n",
    "    plt.imshow(image)\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTensorImage(image):\n",
    "    tfm=transforms.Compose([\n",
    "        # reverse the normalization\n",
    "        transforms.Lambda(lambda x: x.div(255.) ), \n",
    "        transforms.Normalize((-1 * Mean / Std), (1.0 / Std),inplace=True) \n",
    "        ])\n",
    "    img=tfm(image)\n",
    "    plt.figure(figsize=(10,10) )\n",
    "    plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root='archive/butterflies_rev2'\n",
    "\n",
    "trainDS=ImageFolder(f'{root}/train', transform=train_tfms)\n",
    "valDS=ImageFolder(f'{root}/valid', transform=train_tfms)\n",
    "testDS=ImageFolder(f'{root}/test', transform=test_tfms)\n",
    "print(f'train ds has {len(trainDS)},  val ds has {len(valDS) } and testDS has {len(testDS)} items with {len(trainDS.classes)} classes')\n",
    "img, _=trainDS[1]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "num_images=10\n",
    "images = [trainDS[idx][0] for idx in range(num_images)]\n",
    "plt.figure(figsize=(10,10) )\n",
    "\n",
    "image_grid = make_grid(images, nrow=num_images//2)            \n",
    "plt.imshow(image_grid.permute(1, 2, 0).squeeze(0))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(batchSize):\n",
    "    trainldr=DataLoader(trainDS, shuffle=True, num_workers=4, pin_memory=True, batch_size=batchSize ) \n",
    "    valldr=DataLoader(valDS, num_workers=4, pin_memory=True, batch_size=batchSize)\n",
    "    testldr=DataLoader(testDS,num_workers=4, pin_memory=True, batch_size=batchSize)\n",
    "\n",
    "    return trainldr, valldr, testldr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet import ResNet\n",
    "\n",
    "model=ResNet(img, len(trainDS.classes), 0)\n",
    "output=model(img.unsqueeze(0))\n",
    "print(f'output has shape {output.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(outputs, labels):\n",
    "    preds=torch.argmax(outputs, dim=1)\n",
    "    return torch.sum(preds==labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  \n",
    "\n",
    "def train(ldr, opt, sched=None):    \n",
    "    num_items=0\n",
    "    losses=0.\n",
    "    accs=0.\n",
    "    model.train()\n",
    "    for batch in tqdm(ldr):        \n",
    "        opt.zero_grad()        \n",
    "        imgs, lbls=batch \n",
    "        num_items+=len(lbls)\n",
    "        imgs=imgs.to(device)\n",
    "        lbls=lbls.to(device)\n",
    "        outputs=model(imgs)\n",
    "        loss=F.cross_entropy(outputs, lbls)\n",
    "        losses+=(loss.item()*len(lbls))\n",
    "        accs+=getAccuracy(outputs, lbls)\n",
    "        loss.backward()        \n",
    "        opt.step()\n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "    return losses/num_items, accs/num_items \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ldr):\n",
    "    size=0\n",
    "    losses=0. \n",
    "    accs=0. \n",
    "    model.eval()\n",
    "    for batch in tqdm(ldr):\n",
    "        imgs, lbls=batch \n",
    "        size+=len(lbls)\n",
    "        imgs=imgs.to(device )\n",
    "        lbls=lbls.to(device )\n",
    "        outputs=model(imgs)\n",
    "        loss=F.cross_entropy(outputs, lbls)\n",
    "        losses+=(loss.item()*len(lbls))\n",
    "        accs+=getAccuracy(outputs, lbls)\n",
    "    return losses/size , accs/size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "batchsize=512\n",
    "lr=1e-4\n",
    "epochs=50\n",
    "\n",
    "opt=optim.Adam(model.parameters(), lr=lr)\n",
    "trainLdr, valLdr, testLdr =getDataLoaders(batchsize)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(opt,max_lr=lr*10, epochs=epochs,steps_per_epoch=len(trainLdr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  IPython.display as display \n",
    "\n",
    "def showResults(tloss, tacc, vloss, vacc):\n",
    "    display.clear_output(wait=True )\n",
    "    print('epoch\\t TLosses\\t TAccs\\t\\t VLosses\\t VAccs')\n",
    "    print('------------------------------------------------------------')\n",
    "    for i in range(len(tloss)):\n",
    "        print(f'{i+1}\\t {tloss[i]:.3f}\\t\\t {tacc[i]*100.:.3f}% \\t {vloss[i]:.3f}\\t\\t {vacc[i]*100.:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLosses=[]\n",
    "trainAccs=[]\n",
    "valLosses=[]\n",
    "valAccs=[]\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(f'\\nTraining {ep+1}/{epochs}')\n",
    "    losses, accs=train(trainLdr, opt, sched)\n",
    "    trainLosses+=[losses]\n",
    "    trainAccs+=[accs]\n",
    "    print(f'Validating {ep+1}/{epochs}')\n",
    "    losses, accs=evaluate(valLdr)\n",
    "    valLosses+=[losses]\n",
    "    valAccs+=[accs]\n",
    "    showResults(trainLosses, trainAccs, valLosses, valAccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Losses')\n",
    "plt.plot(trainLosses, label='train')\n",
    "plt.plot(valLosses, label='val')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracies')\n",
    "plt.plot(trainAccs, label='train')\n",
    "plt.plot(valAccs, label='val')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accs=evaluate(testLdr)\n",
    "print(f'Test Losses={losses:.3f}, accuracies={accs*100. : .2f} % ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing with images containing various fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img='2.jpg'\n",
    "path=f'{root}/images to predict/{img}'\n",
    "img=loadPicture(path)\n",
    "print(f'iimg shape: {img.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectObject(img):\n",
    "    img=img.to(device)\n",
    "    output=model(img.unsqueeze(0) )\n",
    "    output=F.softmax(output, dim=1)\n",
    "    prob, id=torch.max(output, dim=1)    \n",
    "    return prob.to('cpu') , id.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "prob, id=detectObject(img)\n",
    "prob=prob.detach().numpy()\n",
    "print(f'detected {trainDS.classes[id] } to {prob[0]*100. :.2f}%  ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd5645910c9f12221763b2aa836bb1f3cb241427948cb3afce762a164a0fd337"
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
