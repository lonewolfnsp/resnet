{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification based on butterflies dataset using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import models\n",
    "import torchvision \n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tfm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader \n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset comes from: https://www.kaggle.com/gpiosenka/butterfly-images40-species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the mean and std for ImageNet dataset\n",
    "Mean = np.asarray([ 0.485, 0.456, 0.406 ])\n",
    "Std = np.asarray([ 0.229, 0.224, 0.225 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tfm.Compose([tfm.RandomCrop(224, padding=4, padding_mode='reflect'), \n",
    "                         tfm.RandomHorizontalFlip(0.5),                          \n",
    "                         tfm.RandomVerticalFlip(0.6),\n",
    "                         tfm.ToTensor(), \n",
    "                         tfm.Normalize(mean=Mean, std=Std, inplace=True), # normalize image based on mean and std of ImageNet dataset\n",
    "                         tfm.Lambda(lambda x: x.mul(255.))\n",
    "                         ])\n",
    "test_tfms = tfm.Compose([\n",
    "    tfm.ToTensor(),\n",
    "    tfm.Normalize(mean=Mean, std=Std, inplace=True), # normalize image based on mean and std of ImageNet dataset\n",
    "    tfm.Lambda(lambda x: x.mul(255.))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "load_tfms = tfm.Compose([\n",
    "    tfm.Resize((224,224)), \n",
    "    tfm.ToTensor(),\n",
    "    tfm.Normalize(Mean, Std, inplace=True), # normalize image based on mean and std of ImageNet dataset\n",
    "    tfm.Lambda(lambda x: x.mul(255.))\n",
    "    ])\n",
    "\n",
    "def loadPicture(path):    \n",
    "    image = Image.open(path).convert('RGB')    \n",
    "    img=load_tfms(image)    \n",
    "    plt.imshow(image)\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTensorImage(image):\n",
    "    tfm=transforms.Compose([\n",
    "        # reverse the normalization\n",
    "        transforms.Lambda(lambda x: x.div(255.) ), \n",
    "        transforms.Normalize((-1 * Mean / Std), (1.0 / Std),inplace=True) \n",
    "        ])\n",
    "    img=tfm(image)\n",
    "    plt.figure(figsize=(10,10) )\n",
    "    plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root='archive/butterflies_rev2'\n",
    "\n",
    "trainDS=ImageFolder(f'{root}/train', transform=train_tfms)\n",
    "valDS=ImageFolder(f'{root}/valid', transform=train_tfms)\n",
    "testDS=ImageFolder(f'{root}/test', transform=test_tfms)\n",
    "print(f'train ds has {len(trainDS)},  val ds has {len(valDS) } and testDS has {len(testDS)} items with {len(trainDS.classes)} classes')\n",
    "img, _=trainDS[1]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "num_images=10\n",
    "images = [trainDS[idx][0] for idx in range(num_images)]\n",
    "plt.figure(figsize=(10,10) )\n",
    "\n",
    "image_grid = make_grid(images, nrow=num_images//2)            \n",
    "plt.imshow(image_grid.permute(1, 2, 0).squeeze(0))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(batchSize):\n",
    "    trainldr=DataLoader(trainDS, shuffle=True, num_workers=4, pin_memory=True, batch_size=batchSize ) \n",
    "    valldr=DataLoader(valDS, num_workers=4, pin_memory=True, batch_size=batchSize)\n",
    "    testldr=DataLoader(testDS,num_workers=4, pin_memory=True, batch_size=batchSize)\n",
    "\n",
    "    return trainldr, valldr, testldr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the resnet model from model zoo\n",
    "Transfer learning has 2 modes: \n",
    "1. fine tuning: weights are not frozen abd pretrained model is used as starting point to train for custom data\n",
    "2. feature extractor: weights are frozen. only final linear is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(ntwkType=0, frozen=True):\n",
    "    model=None \n",
    "    \n",
    "    if ntwkType==0:\n",
    "        model=models.resnet18(pretrained=True)        \n",
    "    elif ntwkType==1:        \n",
    "        model=models.resnet34(pretrained=True)\n",
    "    elif ntwkType==2:\n",
    "        model=models.resnet50(pretrained=True)\n",
    "    elif ntwkType==3:\n",
    "        model=models.resnet101(pretrained=True)\n",
    "    elif ntwkType==4:\n",
    "        model=models.resnet152(pretrained=True)        \n",
    "        \n",
    "    inputs=model.fc.in_features\n",
    "    if frozen:        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=False\n",
    "    model.fc=torch.nn.Linear(inputs, len(trainDS.classes) )\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(0)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, path):\n",
    "    torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(outputs, labels):\n",
    "    preds=torch.argmax(outputs, dim=1)\n",
    "    return torch.sum(preds==labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  \n",
    "\n",
    "def train(ldr, opt, sched=None):    \n",
    "    num_items=0\n",
    "    losses=0.\n",
    "    accs=0.\n",
    "    model.train()\n",
    "    for batch in tqdm(ldr):        \n",
    "        opt.zero_grad()        \n",
    "        imgs, lbls=batch \n",
    "        num_items+=len(lbls)\n",
    "        imgs=imgs.to(device)\n",
    "        lbls=lbls.to(device)\n",
    "        outputs=model(imgs)\n",
    "        loss=F.cross_entropy(outputs, lbls)\n",
    "        losses+=(loss.item()*len(lbls))\n",
    "        accs+=getAccuracy(outputs, lbls)\n",
    "        loss.backward()        \n",
    "        opt.step()\n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "    return losses/num_items, accs/num_items \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ldr, trainedModel):\n",
    "    size=0\n",
    "    losses=0. \n",
    "    accs=0. \n",
    "    trainedModel.eval()\n",
    "    for batch in tqdm(ldr):\n",
    "        imgs, lbls=batch \n",
    "        size+=len(lbls)\n",
    "        imgs=imgs.to(device )\n",
    "        lbls=lbls.to(device )\n",
    "        outputs=trainedModel(imgs)\n",
    "        loss=F.cross_entropy(outputs, lbls)\n",
    "        losses+=(loss.item()*len(lbls))\n",
    "        accs+=getAccuracy(outputs, lbls)\n",
    "    return losses/size , accs/size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "batchsize=512\n",
    "lr=1e-3\n",
    "epochs=20\n",
    "\n",
    "opt=optim.Adam(model.parameters(), lr=lr)\n",
    "trainLdr, valLdr, testLdr =getDataLoaders(batchsize)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(opt,max_lr=lr*2.5, epochs=epochs,steps_per_epoch=len(trainLdr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  IPython.display as display \n",
    "\n",
    "def showResults(tloss, tacc, vloss, vacc):\n",
    "    display.clear_output(wait=True )\n",
    "    print('epoch\\t TLosses\\t TAccs\\t\\t VLosses\\t VAccs')\n",
    "    print('------------------------------------------------------------')\n",
    "    for i in range(len(tloss)):\n",
    "        print(f'{i+1}\\t {tloss[i]:.3f}\\t\\t {tacc[i]*100.:.3f}% \\t {vloss[i]:.3f}\\t\\t {vacc[i]*100.:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLosses=[]\n",
    "trainAccs=[]\n",
    "valLosses=[]\n",
    "valAccs=[]\n",
    "curValAcc=0.\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(f'\\nTraining {ep+1}/{epochs}')\n",
    "    losses, accs=train(trainLdr, opt, sched)\n",
    "    trainLosses+=[losses]\n",
    "    trainAccs+=[accs]\n",
    "    print(f'Validating {ep+1}/{epochs}')\n",
    "    losses, accs=evaluate(valLdr, model)\n",
    "    valLosses+=[losses]\n",
    "    valAccs+=[accs]\n",
    "    if accs>curValAcc:\n",
    "        save(model, 'model.pth')\n",
    "        curValAcc=accs\n",
    "    showResults(trainLosses, trainAccs, valLosses, valAccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Losses')\n",
    "plt.plot(trainLosses, label='train')\n",
    "plt.plot(valLosses, label='val')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracies')\n",
    "plt.plot(trainAccs, label='train')\n",
    "plt.plot(valAccs, label='val')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the best performing weights from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=torch.load('model.pth')\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accs=evaluate(testLdr,model2)\n",
    "print(f'Test Losses={losses:.3f}, accuracies={accs*100. : .2f} % ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing with images containing various fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img='2.jpg'\n",
    "path=f'{root}/images to predict/{img}'\n",
    "img=loadPicture(path)\n",
    "print(f'iimg shape: {img.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectObject(img, model):\n",
    "    img=img.to(device)\n",
    "    output=model(img.unsqueeze(0) )\n",
    "    output=F.softmax(output, dim=1)\n",
    "    prob, id=torch.max(output, dim=1)    \n",
    "    return prob.to('cpu') , id.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "prob, id=detectObject(img,model2)\n",
    "prob=prob.detach().numpy()\n",
    "print(f'detected {trainDS.classes[id] } to {prob[0]*100. :.2f}%  ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd5645910c9f12221763b2aa836bb1f3cb241427948cb3afce762a164a0fd337"
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
